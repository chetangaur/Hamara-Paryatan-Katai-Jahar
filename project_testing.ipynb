{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "project_testing.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Abhinaay/Hamara-Paryatan-Katai-Jahar/blob/master/project_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cv24rdhcEDTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!chmod +x detection.py"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-wySfXuFNslG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "\n",
        "def take_photo(filename='photo.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      //const capture = document.createElement('button');\n",
        "      //capture.textContent = 'Capture';\n",
        "      //div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => 1);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17qUAXNTNsgZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 949
        },
        "outputId": "ded90161-6d2b-423e-9559-6aeb41ac5669"
      },
      "source": [
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import cv2\n",
        "import getch\n",
        "import time\n",
        "from IPython.display import Image\n",
        "\n",
        "\n",
        "#video=take_photo #this variable either contain path to the video for processing or either camera input\n",
        "ranger=3 #This variable defines no of which must increase or decrease in the frame to change the output\n",
        "\n",
        "class DetectorAPI:\n",
        "    def __init__(self, path_to_ckpt):\n",
        "        self.path_to_ckpt = path_to_ckpt\n",
        "\n",
        "        self.detection_graph = tf.Graph()\n",
        "        with self.detection_graph.as_default():\n",
        "            od_graph_def = tf.compat.v1.GraphDef()\n",
        "            with tf.io.gfile.GFile(self.path_to_ckpt, 'rb') as fid:\n",
        "                serialized_graph = fid.read()\n",
        "                od_graph_def.ParseFromString(serialized_graph)\n",
        "                tf.import_graph_def(od_graph_def, name='')\n",
        "\n",
        "        self.default_graph = self.detection_graph.as_default()\n",
        "        self.sess = tf.compat.v1.Session(graph=self.detection_graph)\n",
        "\n",
        "        # Definite input and output Tensors for detection_graph\n",
        "        self.image_tensor = self.detection_graph.get_tensor_by_name('image_tensor:0')\n",
        "        # Each box represents a part of the image where a particular object was detected.\n",
        "        self.detection_boxes = self.detection_graph.get_tensor_by_name('detection_boxes:0')\n",
        "        # Each score represent how level of confidence for each of the objects.\n",
        "        # Score is shown on the result image, together with the class label.\n",
        "        self.detection_scores = self.detection_graph.get_tensor_by_name('detection_scores:0')\n",
        "        self.detection_classes = self.detection_graph.get_tensor_by_name('detection_classes:0')\n",
        "        self.num_detections = self.detection_graph.get_tensor_by_name('num_detections:0')\n",
        "\n",
        "    def processFrame(self, image):\n",
        "        # Expand dimensions since the trained_model expects images to have shape: [1, None, None, 3]\n",
        "        image_np_expanded = np.expand_dims(image, axis=0)\n",
        "        # Actual detection.\n",
        "        start_time = time.time()\n",
        "        (boxes, scores, classes, num) = self.sess.run(\n",
        "            [self.detection_boxes, self.detection_scores, self.detection_classes, self.num_detections],\n",
        "            feed_dict={self.image_tensor: image_np_expanded})\n",
        "        end_time = time.time()\n",
        "\n",
        "        #print(\"Elapsed Time:\", end_time-start_time)\n",
        "\n",
        "        im_height, im_width,_ = image.shape\n",
        "        boxes_list = [None for i in range(boxes.shape[1])]\n",
        "        for i in range(boxes.shape[1]):\n",
        "            boxes_list[i] = (int(boxes[0,i,0] * im_height),\n",
        "                        int(boxes[0,i,1]*im_width),\n",
        "                        int(boxes[0,i,2] * im_height),\n",
        "                        int(boxes[0,i,3]*im_width))\n",
        "\n",
        "        return boxes_list, scores[0].tolist(), [int(x) for x in classes[0].tolist()], int(num[0])\n",
        "\n",
        "    def close(self):\n",
        "        self.sess.close()\n",
        "        self.default_graph.close()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    model_path = 'frozen_inference_graph.pb'\n",
        "    odapi = DetectorAPI(path_to_ckpt=model_path)\n",
        "    threshold = 0.7\n",
        "    #cap = take_photo()\n",
        "    print(\"Press ESC key to exit the program \")\n",
        "    print(\"\\n\\n\")\n",
        "    #choose=input(\"Visual Output Required(y) or not(n) \")\n",
        "    old=0\n",
        "    print(\"No of Humans are: 0\")\n",
        "    #if choose.lower()=='y':\n",
        "    while True:\n",
        "        img = take_photo()\n",
        "        img = cv2.imread('photo.jpg')\n",
        "        img = cv2.resize(img, (1280, 720))\n",
        "\n",
        "        boxes, scores, classes, num = odapi.processFrame(img)\n",
        "            \n",
        "            # Visualization of the results of a detection\n",
        "        count=0\n",
        "        for i in range(len(boxes)):\n",
        "                # Class 1 represents human\n",
        "            if classes[i] == 1 and scores[i] > threshold:\n",
        "                box = boxes[i]\n",
        "                cv2.rectangle(img,(box[1],box[0]),(box[3],box[2]),(255,0,255),2)\n",
        "                count+=1\n",
        "            if old==0 and count>0:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            elif count>=old+ranger:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            elif count<old:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            #display(\"preview\", Image(img))\n",
        "            if cv2.waitKey(0)==13:\n",
        "                #if ord(getch.getch()) == 13:\n",
        "                break\n",
        "    cv2.destroyAllWindows()\n",
        "    cap.release()\n",
        "    print(\"\\n\\n\\tBYEBYE\\n\\n\")\n",
        "'''\n",
        "    elif choose.lower()=='n':\n",
        "        while True:\n",
        "            img = take_photo()\n",
        "            img = cv2.imread('photo.jpg')\n",
        "            img = cv2.resize(img, (1280, 720))\n",
        "\n",
        "            boxes, scores, classes, num = odapi.processFrame(img)\n",
        "            count=0\n",
        "            for i in range(len(boxes)):\n",
        "                if classes[i] == 1 and scores[i] > threshold:\n",
        "                    count+=1\n",
        "            if old==0 and count>0:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            elif count>=old+ranger:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            elif count<old:\n",
        "                print(\"No of Humans are:\",count)\n",
        "                old=count\n",
        "            if cv2.waitKey(0)==13:\n",
        "                break\n",
        "'''                \n",
        "                  \n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Press ESC key to exit the program \n",
            "\n",
            "\n",
            "\n",
            "No of Humans are: 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function takePhoto(quality) {\n",
              "      const div = document.createElement('div');\n",
              "      //const capture = document.createElement('button');\n",
              "      //capture.textContent = 'Capture';\n",
              "      //div.appendChild(capture);\n",
              "\n",
              "      const video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
              "\n",
              "      document.body.appendChild(div);\n",
              "      div.appendChild(video);\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      // Resize the output to fit the video element.\n",
              "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
              "\n",
              "      // Wait for Capture to be clicked.\n",
              "      await new Promise((resolve) => 1);\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      canvas.width = video.videoWidth;\n",
              "      canvas.height = video.videoHeight;\n",
              "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
              "      stream.getVideoTracks()[0].stop();\n",
              "      div.remove();\n",
              "      return canvas.toDataURL('image/jpeg', quality);\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-75050f6c1998>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;31m#if choose.lower()=='y':\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtake_photo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'photo.jpg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1280\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m720\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-7d892d55e7cb>\u001b[0m in \u001b[0;36mtake_photo\u001b[0;34m(filename, quality)\u001b[0m\n\u001b[1;32m     36\u001b[0m     ''')\n\u001b[1;32m     37\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m   \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_js\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'takePhoto({})'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquality\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m   \u001b[0mbinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mb64decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result)\u001b[0m\n\u001b[1;32m     37\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     if (reply.get('type') == 'colab_reply' and\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97pGw3LeQHbl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jh3k4M5jOZup",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}